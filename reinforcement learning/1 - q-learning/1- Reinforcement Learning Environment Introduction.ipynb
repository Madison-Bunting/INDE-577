{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Reinforcement Learning Environment\n",
    "This notebook will introduce the reinforcement learning environment that will be used in the [Q-learning notebook](https://github.com/Madison-Bunting/INDE-577/blob/main/reinforcement%20learning/1%20-%20q-learning/2%20-%20Q-learning.ipynb), and how it can be used as a metaphor for a common problem in Human Resources Analytics: help companies provide employees with the right amount and kinds of training to maximize the employees' capability and productivity, while minimizing cost to the company and maintaining employee satisfaction. \n",
    "\n",
    "This environment was created in the [OpenAi gym environment](https://gym.openai.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the gym environment \n",
    "import gym\n",
    "\n",
    "# Instantiate the taxi environment \n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# Reset the environment \n",
    "env.reset()\n",
    "\n",
    "# Show the current frame of the environment \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Taxi-v3 environment (and translation to the context of employee training)\n",
    "\n",
    "There are 4 locations (labeled by different letters), with the goal of picking a passenger up at one location and dropping them off at another. In the case of employee training, each location represents a level of knowledge the employee has on the topic of the training. Pickup locations signify the amount of prior knowledge that the employee enters the training program with, and dropoff locations signify the \"goal\" level of knowledge the employer hopes the employee has by the end of the program. \n",
    "\n",
    "* **Observations**: There are 500 discrete states since there are 25 training opportunities (*i.e. taxi positions*), 5 knowledge states, including while the passenger is in-training (*i.e. locations of the passenger, including the case when the passenger is in the taxi*), and 4 goal levels of knowledge (*i.e. destination locations*). \n",
    "   \n",
    "* **Employee knowledge states** (*passenger locations*):\n",
    " - 0: R(ed)\n",
    " - 1: G(reen)\n",
    " - 2: Y(ellow)\n",
    " - 3: B(lue)\n",
    " - 4: in training (*i.e. taxi*)\n",
    "  \n",
    "* **Goal levels of employee knowledge** (*Destinations*):\n",
    " - 0: R(ed)\n",
    " - 1: G(reen)\n",
    " - 2: Y(ellow)\n",
    " - 3: B(lue)\n",
    "   \n",
    "* **Actions**: There are 6 discrete deterministic actions, 4 types of training (*i.e. taxi movement in each of the 4 directions*) and employees entering/completing the training program:\n",
    " - 0: move south\n",
    " - 1: move north\n",
    " - 2: move east\n",
    " - 3: move west\n",
    " - 4: an employee received a note in their performance evaluation that they need to receive training (*i.e. pickup passenger*)\n",
    " - 5: the employee successfully completed their assigned training program (*i.e. drop off passenger*)\n",
    " \n",
    "* **Rewards**: \n",
    "- Each successfully trained employee (*i.e. passenger that is dropped off at the correct location*) increases company revenue by 20,000 dollars per year.\n",
    "- Each training the employee must attend (*i.e. time-step*) costs the company 1,000 dollars\n",
    "- Each time the employee is incorrectly released from the training program (*i.e. illegal pick-up and drop-off actions*) costs the company 10,000 dollars because the employee was not performing their usual role while in training, and is dissatisfied with the company having \"wasted their time\" so they are less productive\n",
    "\n",
    "* **Rendering**:\n",
    " - blue: passenger\n",
    " - magenta: destination\n",
    " - yellow: empty taxi\n",
    " - green: full taxi\n",
    " - other letters (R, G, Y and B): locations for passengers and destinations\n",
    "   \n",
    "* state space is represented by: (taxi_row, taxi_col, passenger_location, destination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "action: 5\n",
      "observation: 393\n",
      "company revenue due to training (in thousands of dollars): -10\n",
      "done: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Choose a random action\n",
    "action = np.random.randint(6)\n",
    "\n",
    "# Take a step\n",
    "observation, reward, done, info = env.step(action)\n",
    "\n",
    "# Show the current frame of the environment \n",
    "env.render()\n",
    "\n",
    "print(f\"\\naction: {action}\")\n",
    "print(f\"observation: {observation}\")\n",
    "print(f\"company revenue due to training (in thousands of dollars): {reward}\")\n",
    "print(f\"done: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also manually set the state of the current ``env`` by first using the ``env.encode`` method, then setting the current state (``env.s``) to be this new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 48\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "# Use the env.encode method \n",
    "state = env.encode(0, 2, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(f\"State: {state}\")\n",
    "\n",
    "# Set the current state of the environmnet \n",
    "env.s = state\n",
    "\n",
    "# Show the current frame of the environment \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code resets the environment and creates an initial assessment of how many employees out of 150 receive an incorrect number of trainings (the number differs each time the code is run, but the average is around 1/3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of actions (giving trainings, adding employees to the training program or releasing employees from the program): 150 \n",
      "\n",
      "Number of times employees were given an incorrect number of trainings: 53 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "epochs = 0\n",
    "penalties = 0\n",
    "reward = 0\n",
    "max_iter = 150\n",
    "frames = [] # for animation\n",
    "done = False\n",
    "\n",
    "while not done and epochs < max_iter:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({'frame': env.render(mode='ansi'),\n",
    "                   'state': state,\n",
    "                   'action': action,\n",
    "                   'reward': reward})\n",
    "    epochs += 1\n",
    "       \n",
    "print(f\"Total number of actions (giving trainings, adding employees to the training program or releasing employees from the program): {epochs} \\n\")\n",
    "print(f\"Number of times employees were given an incorrect number of trainings: {penalties} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell visualizes what is going on when the code above runs. The company consistently loses at least $500,000 in trying to train its employees without the reinforcement learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "Total number of actions to date: 150\n",
      "Current State: 309\n",
      "Action selected: 0\n",
      "Company revenue due to training for that action (in thousands of dollars): -1\n",
      "To date company revenue due to training (in thousands of dollars): -627\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    total_cost = 0\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        #print(frame['frame'].getvalue())\n",
    "        print(f\"Total number of actions to date: {i + 1}\")\n",
    "        print(f\"Current State: {frame['state']}\")\n",
    "        print(f\"Action selected: {frame['action']}\")\n",
    "        print(f\"Company revenue due to training for that action (in thousands of dollars): {frame['reward']}\")\n",
    "        total_cost += frame['reward']\n",
    "        print(f\"To date company revenue due to training (in thousands of dollars): {total_cost}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 450, -1, False)],\n",
       " 1: [(1.0, 350, -1, False)],\n",
       " 2: [(1.0, 450, -1, False)],\n",
       " 3: [(1.0, 430, -1, False)],\n",
       " 4: [(1.0, 450, -10, False)],\n",
       " 5: [(1.0, 450, -10, False)]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[450]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
