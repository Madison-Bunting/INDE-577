{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "This notebook contains an application of the TaxiCab reinforcement learning environment used as a metaphor for employee training programs, with the goal of maximizing profit from increased employee learning and minimizing cost of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the gym environment \n",
    "import gym\n",
    "\n",
    "# Instantiate the taxi environment \n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# Reset the environment \n",
    "env.reset()\n",
    "\n",
    "# Show the current frame of the environment \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Taxi-v3 environment (and translation to the context of employee training)\n",
    "\n",
    "There are 4 locations (labeled by different letters), with the goal of picking a passenger up at one location and dropping them off at another. In the case of employee training, each location represents a level of knowledge the employee has on the topic of the training. Pickup locations signify the amount of prior knowledge the employee enters the training with, and dropoff locations signify the \"goal\" level of knowledge the employer hopes the employee has by the end of the program. \n",
    "\n",
    "* **Observations**: There are 500 discrete states since there are 25 training opportunities (*i.e. taxi positions*), 5 knowledge states, including while the passenger is in-training (*i.e. locations of the passenger, including the case when the passenger is in the taxi*), and 4 goal levels of knowledge (*i.e. destination locations*). \n",
    "   \n",
    "* **Employee knowledge states** (*passenger locations*):\n",
    " - 0: R(ed)\n",
    " - 1: G(reen)\n",
    " - 2: Y(ellow)\n",
    " - 3: B(lue)\n",
    " - 4: in training (*i.e. taxi*)\n",
    "  \n",
    "* **Goal levels of employee knowledge** (*Destinations*):\n",
    " - 0: R(ed)\n",
    " - 1: G(reen)\n",
    " - 2: Y(ellow)\n",
    " - 3: B(lue)\n",
    "   \n",
    "* **Actions**: There are 6 discrete deterministic actions, 4 types of training (*i.e. taxi movement in each of the 4 directions*) and employees entering/completing the training program:\n",
    " - 0: move south\n",
    " - 1: move north\n",
    " - 2: move east\n",
    " - 3: move west\n",
    " - 4: an employee received a note in their performance evaluation that they need to receive training (*i.e. pickup passenger*)\n",
    " - 5: the employee successfully completed their assigned training program (*i.e. drop off passenger*)\n",
    " \n",
    "* **Rewards**: \n",
    "- Each successfully trained employee (*i.e. passenger that is dropped off at the correct location*) increases company revenue by 20,000 dollars per year.\n",
    "- Each training the employee must attend (*i.e. time-step*) costs the company 1,000 dollars\n",
    "- Each time the employee is incorrectly released from the training program (*i.e. illegal pick-up and drop-off actions*) costs the company 10,000 dollars\n",
    "\n",
    "* **Rendering**:\n",
    " - blue: passenger\n",
    " - magenta: destination\n",
    " - yellow: empty taxi\n",
    " - green: full taxi\n",
    " - other letters (R, G, Y and B): locations for passengers and destinations\n",
    "   \n",
    "* state space is represented by: (taxi_row, taxi_col, passenger_location, destination)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "action: 5\n",
      "observation: 348\n",
      "company spending (in thousands of dollars): -10\n",
      "done: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Choose a random action\n",
    "action = np.random.randint(6)\n",
    "\n",
    "# Take a step\n",
    "observation, reward, done, info = env.step(action)\n",
    "\n",
    "# Show the current frame of the environment \n",
    "env.render()\n",
    "\n",
    "print(f\"\\naction: {action}\")\n",
    "\n",
    "print(f\"observation: {observation}\")\n",
    "\n",
    "print(f\"company spending (in thousands of dollars): {reward}\")\n",
    "\n",
    "print(f\"done: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can also manually set the state of the current ``env`` by first using the ``env.encode`` method and then setting the current state (``env.s``) to be this new state. The following code cell illustrates this option. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 48\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "# Use the env.encode method \n",
    "state = env.encode(0, 2, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(f\"State: {state}\")\n",
    "\n",
    "# Set the current state of the environmnet \n",
    "env.s = state\n",
    "\n",
    "# Show the current frame of the environment \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of actions (giving trainings, adding employees to the training program or releasing employees from the program): 150 \n",
      "\n",
      "Number of times employees were given an incorrect number of trainings: 39 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "epochs = 0\n",
    "penalties = 0\n",
    "reward = 0\n",
    "max_iter = 150\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done and epochs < max_iter:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(f\"Total number of actions (giving trainings, adding employees to the training program or releasing employees from the program): {epochs} \\n\")\n",
    "print(f\"Number of times employees were given an incorrect number of trainings: {penalties} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[43mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "Total number of actions to date: 150\n",
      "State: 14\n",
      "Action: 1\n",
      "Company spending for that action (in thousands of dollars): -1\n",
      "To date company spending on training (in thousands of dollars): -501\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    total_cost = 0\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        #print(frame['frame'].getvalue())\n",
    "        print(f\"Total number of actions to date: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Company spending for that action (in thousands of dollars): {frame['reward']}\")\n",
    "        total_cost += frame['reward']\n",
    "        print(f\"To date company spending on training (in thousands of dollars): {total_cost}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "\n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 450, -1, False)],\n",
       " 1: [(1.0, 350, -1, False)],\n",
       " 2: [(1.0, 450, -1, False)],\n",
       " 3: [(1.0, 430, -1, False)],\n",
       " 4: [(1.0, 450, -10, False)],\n",
       " 5: [(1.0, 450, -10, False)]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
