{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "This notebook contains an application of the TaxiCab reinforcement learning environment used as a metaphor for employee training programs, with the goal of maximizing profit from increased employee learning and minimizing cost of training. The environment that will be used is explained in [this](https://github.com/Madison-Bunting/INDE-577/blob/main/reinforcement%20learning/1%20-%20q-learning/1-%20Reinforcement%20Learning%20Environment%20Introduction.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This environment already has the actions and states defined along with the rewards, and creating the environment also creates the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the gym environment \n",
    "import gym\n",
    "\n",
    "# Instantiate the taxi environment \n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# Reset the environment \n",
    "env.reset()\n",
    "\n",
    "# Show the current frame of the environment \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Q-table to keep track of the agent's options and potential rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the Q-table is: (500, 6) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize Q-values as a Q-table of 0's \n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "print(f\"The shape of the Q-table is: {q_table.shape} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train and validate the agent by running it through \"episodes\", defined as each time the environment resets. In this case, it would be the company restarting its training program from scratch and running on a batch of new employees. Note that this takes time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10000\n",
      "Training finished.\n",
      "\n",
      "State: 44\n",
      "Q-table: [ -2.46538759  -2.44496635  -2.41837066  -2.44931071 -10.99548104\n",
      " -10.59684768]\n",
      "0\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "frames = []\n",
    "for i in range(1, 10_001):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        # Q-table update Rule \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "       \n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "       \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "print(\"Training finished.\\n\")\n",
    "\n",
    "#Visualize the process\n",
    "state = env.encode(0, 2, 1, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "print(\"Q-table:\", q_table[state])\n",
    "env.s = state\n",
    "print(np.argmax(state))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we execute the policy. Notice that in comparison to when the environment was first created in the [environment notebook](https://github.com/Madison-Bunting/INDE-577/blob/main/reinforcement%20learning/1%20-%20q-learning/1-%20Reinforcement%20Learning%20Environment%20Introduction.ipynb), this agent almost never makes errors. In the context of a training program, this means the program efficiently provides employees with the fewest number of trainings to maximize their performance, and the company makes a net positive from the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after training 10 employees:\n",
      "Average number of trainings each employee needed to participate in: 12.5\n",
      "Average number of incorrect trainings per employee: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 10\n",
    "frames = []\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({'frame': env.render(mode='ansi'),\n",
    "                       'state': state,\n",
    "                       'action': action,\n",
    "                       'reward': reward})\n",
    "        epochs += 1\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after training {episodes} employees:\")\n",
    "print(f\"Average number of trainings each employee needed to participate in: {total_epochs / episodes}\")\n",
    "print(f\"Average number of incorrect trainings per employee: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Total number of actions to date: 125\n",
      "State: 475\n",
      "Action: 5\n",
      "Company revenue due to training for that action (in thousands of dollars): 20\n",
      "To date company revenue due to training (in thousands of dollars): 85\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    total_cost = 0\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        #print(frame['frame'].getvalue())\n",
    "        print(f\"Total number of actions to date: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Company revenue due to training for that action (in thousands of dollars): {frame['reward']}\")\n",
    "        total_cost += frame['reward']\n",
    "        print(f\"To date company revenue due to training (in thousands of dollars): {total_cost}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that on average, employees need to participate in between 10 and 15 trainings to \"level up\" their skills, and the company nets $20,000-25,000 in additional revenue per employee they train.\n",
    "\n",
    "This analysis could be used by HR partners to justify their training program. In the future, the model could additionally incorporate:\n",
    "- different types of trainings\n",
    "- more specific employee starting states (via a pre-test)\n",
    "- the impact of combined training regimens (for example, DEI training and technical skills training)\n",
    "- comparisons for the cost vs outcomes (in dollar amounts) of different training providers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
